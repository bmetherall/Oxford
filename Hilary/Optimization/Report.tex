\documentclass[11pt,a4paper,twocolumn]{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{graphicx}
%\usepackage[subfolder,cleanup]{gnuplottex}
%\usepackage{amsthm}
%\usepackage{enumitem}
%\usepackage{wrapfig}
\usepackage{subcaption}
%\usepackage{hyperref}
%\usepackage{tikz}
\usepackage{makecell}
%\usepackage{dblfloatfix}

\usepackage{lipsum}

% Nicer brackets for operators
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

% Math operators
\providecommand{\bigO}[1]{\ensuremath{\mathop{}\mathopen{}\mathcal{O}\mathopen{}\left(#1\right)}}

% Macros
\newcommand{\diff}[3][\hspace{-0.5pt}]{\frac{\textrm{d}^{#1}#2}{\textrm{d}{#3}^{#1}}}
\newcommand{\pdiff}[3][\hspace{-0.5pt}]{\frac{\partial^{#1}#2}{\partial{#3}^{#1}}}
\newcommand{\df}{\, \textrm{d}}
%\newcommand{\eps}{\varepsilon}

% Row colouring in tables
%\usepackage[table]{xcolor}
%\rowcolors{2}{gray!25}{white}

% Margin size
\newgeometry{margin=2cm}

%\renewcommand\bottomfraction{.75}

% Reference style
%\bibliographystyle{ieeetr}

\title{Continuous Optimization \\ InFoMM Assignment}
\author{Brady Metherall}
\date{16 March, 2020}

\begin{document}
\maketitle

We explore numerically solving unconstrained and constrained optimization problems using the solve KNITRO using the Julia interface. First we examine the Rosenbrock function, the well-known test problem. We then investigate the discrete catenary problem and how the solution relates to the continuous case.

\section{Rosenbrock Function}
We begin by using KNITRO to solve a well known test problem in optimization---minimizing the Rosenbrock function:
\begin{align}
	\min_{x, y \in \mathbb{R}} 100(y - x^2)^2 + (1 - x)^2.
	\label{eq:rosen}
\end{align}
This function is useful to test minimization algorithms with because it has a unique minimum, $(1,1)$, but has a very small gradient within a valley. The Rosenbrock function is plotted in Figure \ref{fig:rosenunconst}. Additionally, we impose the constraints 
\begin{equation}
	\begin{aligned}
		(x - 1)^2 + y &\geq 1, \\
		x + y &\leq 2,
	\end{aligned}
	\label{eq:rosenconst}
\end{equation}
to further test KNITRO. With these constraints the minimum $(1,1)$ is still feasible, but at the boundary (Figure \ref{fig:rosenconst}).

\begin{figure}[tb]
	\centering
	\begin{subfigure}{\columnwidth}
		\centering
		\input{Rosen}
		\caption{Unconstrained.}
		\label{fig:rosenunconst}
	\end{subfigure} \\
	\begin{subfigure}{\columnwidth}
		\centering
		\input{RosenConst}
		\caption{Constrained by \eqref{eq:rosenconst}.}
		\label{fig:rosenconst}
	\end{subfigure}
	\caption{Rosenbrock function given by \eqref{eq:rosen}.}
	\label{fig:rosen}
\end{figure}

Both the unconstrained and constrained minimization problems are solved with KNITRO for a variety of initial points with a range of derivative information given. The number of iterations required to find the solution $(1,1)$ are shown in Table \ref{tab:iterations}. Generally, providing the analytic gradient does not reduce the number of iterations required to converge---especially for the starting values of $(0,0)$ and $(2,4)$ where the solver does not converge. It is suspected this is because these points lay on the line $y = x^2$, and so one of the partial derivatives vanish. However, by providing the Hessian the number of iterations decreases substantially most of the time. Interestingly, the unconstrained problem requires more iterations that the constrained one.

\begin{table*}[tbp]
	\centering
	\begin{tabular}{lcccccc}
		Starting point & $(-2,-3)$ & $(2,1)$ & $(-3,4)$ & $(\pi,-\textrm{e})$ & $(0,0)$ & $(2,4)$ \\
		\noalign{\smallskip}\Xhline{2.5\arrayrulewidth}\hline\noalign{\smallskip}
		\multicolumn{7}{c}{Unconstrained} \\
		\noalign{\smallskip}\hline\noalign{\smallskip}
		No derivatives & 15 & 43 & 28 & 35 & 22 & 35 \\
		Gradient & 15 & 41 & 28 & 35 & DNC\footnotemark & DNC \\
		Hessian & 25 & 13 & 28 & 20 & DNC & DNC \\
		\noalign{\smallskip}\hline\noalign{\smallskip}
		\multicolumn{7}{c}{Constrained} \\
		\noalign{\smallskip}\hline\noalign{\smallskip}
		No derivatives & 36 & 16 & 40 & 22 & 17 & 18 \\
		Gradient & 36 & 16 & 41 & 22 & DNC & DNC \\
		Hessian & 20 & 12 & 23 & 12 & DNC & DNC \\
		\noalign{\smallskip}\hline\noalign{\smallskip}
	\end{tabular} \\
	\footnotesize{$^1$Did not converge.}
	\caption{Iterations needed to converge to the minimum of the constrained and unconstrained Rosenbrock function.}
	\label{tab:iterations}
\end{table*}

With an understanding of how to use KNITRO, we move onto a more exciting problem.

\section{Catenary Problem}
We now turn our attention to the catenary problem. The catenary problem is concerned with the shape of a freely hanging rope with fixed endpoints neglecting bending stiffness. The solution is the shape that minimizes the gravitational potential energy, and thus is a staple in calculus of variations and mechanics courses. In this section we focus our attention on the discrete analogue of this problem. We instead consider a series of $n$ rigid beams attached together. By assuming the gravitational potential energy is focused entirely at the centre of each beam, we obtain the expression
\begin{align}
	U = m g \left( \frac{1}{2} y_0 + \frac{1}{2} y_n + \sum_{i = 1}^{n-1} y_i \right)
\end{align}
for the total potential energy, as the midpoint is the average of the endpoints of each beam. Additionally, there are multiple constraints of the system. We assume both endpoints are fixed at the same height, and that each beam is the same length, $L$. Combining these constraints with the expression for the potential energy yields the optimization problem
\begin{gather}
	\min_{\mathbf{x}, \mathbf{y} \in \mathbb{R}^{n+1}} m g \left( \frac{1}{2} y_0 + \frac{1}{2} y_n + \sum_{i = 1}^{n-1} y_i \right) \\
	\begin{align}
		x_0 &= 0, & x_n &= \gamma n L, \nonumber \\
		y_0 &=0, & y_n &= 0,
	\end{align} \\
	(x_i - x_{i+1})^2 + (y_i - y_{i+1})^2 = L^2, \nonumber
\end{gather}
where $\gamma$ is the ratio of the width of the span to the total length of the beams. While solving the problem we use the the following numerical values:
\begin{gather}
	\begin{align}
		x_n &= 10, & g &= 1, & m &= 1,
	\end{align} \\
	\begin{align}
		\gamma &= \frac{4}{5}, & L &= \frac{x_n}{\gamma n}.
	\end{align}
\end{gather}

\begin{figure*}[tbp]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\input{Beam5}
		\caption{$n = 4$.}
		\label{fig:beam4}
	\end{subfigure} \\
	\begin{subfigure}{\textwidth}
		\centering
		\input{Beam11}
		\caption{$n = 10$.}
		\label{fig:beam10}	
	\end{subfigure}
	\caption{Solution of the discrete catenary problem.}
	\label{fig:catenary}
\end{figure*}

The solutions from KNITRO for two values of $n$ can be found in Figure \ref{fig:catenary}. We shall compare our results to the solution of the continuous case, given by
\begin{align}
	f(x) = a \left( \cosh \left( \frac{2x - x_n}{2a} \right) - \cosh \left( \frac{x_n}{2a} \right) \right),
\end{align}
where $a$ is the positive solution to
\begin{align}
	\frac{x_n}{2a} &= \gamma \sinh \left( \frac{x_n}{2a} \right).
\end{align}
We see that for $n = 4$ (Figure \ref{fig:beam4}) the discrete solution closely approximates the continuous solution. Moreover, it is evident from $n = 10$ (Figure \ref{fig:beam10}) that as $n \rightarrow \infty$ the continuous solution will be obtained. This observation is justified by examining the $L^2$ norm of the difference of the two solutions for a range of number of beams---the result of this can be seen in Figure \ref{fig:convergence}. We find that the discrete solution converges to the continuous solution approximately quadratically. However, when $n \gtrsim 140$ KNITRO is no longer able to solve the optimization problem since the number of variables is approaching $300$.

\begin{figure}[tbp]
	\centering
	\input{Convergence}
	\caption{The $L^2$ norm of the difference between the discrete solution and the continuous solution.}
	\label{fig:convergence}
\end{figure}

\section{Conclusion}

%\bibliography{Ref}

\end{document}
